{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from clip_model import CLIP\n",
    "from vae.vae_model import VAE_Decoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda:7')\n",
    "# CLIP model\n",
    "model_root = '../checkpoints/clip_7/clip_best.pth'\n",
    "clip_model = CLIP(embed_dim=64)\n",
    "clip_model_weight = torch.load(model_root, map_location=device)\n",
    "clip_model.load_state_dict(clip_model_weight)\n",
    "clip_model = clip_model.to(device)\n",
    "\n",
    "# VAE\n",
    "decoder = VAE_Decoder()\n",
    "# VAE_path\n",
    "vae_path = '../checkpoints/vae_1/VAE_model_ep9.pth'\n",
    "checkpoint = torch.load(vae_path, map_location=device)\n",
    "decoder.load_state_dict(checkpoint['decoder'])\n",
    "decoder = decoder.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.ptbxl_dataset import PtbxlDataset_VAE\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "PTB_VAE_PATH = '/data/0shared/laiyongfan/data_text2ecg/ptb-xl_vae'\n",
    "dataset = PtbxlDataset_VAE(PTB_VAE_PATH)\n",
    "dataloader = DataLoader(dataset, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = pd.read_csv('/data/0shared/chenjiabo/DiffuSETS/data/ptbxl_database_embed.csv', low_memory=False)[['ecg_id', 'text_embed']]\n",
    "original_sheet = pd.read_csv('/data/0shared/laiyongfan/data_text2ecg/ptb-xl/ptbxl_database.csv', low_memory=False)\n",
    "embedding_dict = pd.merge(embedding_dict, original_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_text_embedding_ptbxl(text:str):\n",
    "    text = text.split('|')[0]\n",
    "    text = text.replace('The report of the ECG is that ', '')\n",
    "    try:\n",
    "        text_embed = embedding_dict.loc[embedding_dict['report'] == text, 'text_embed'].values[0]\n",
    "        text_embed = eval(text_embed)\n",
    "    except IndexError:\n",
    "        text_embed = [0] * 1536\n",
    "        # print(text)\n",
    "    return torch.tensor(text_embed)\n",
    "\n",
    "@torch.no_grad()\n",
    "def CLIP_Score(clip_model, test_dataloader, fetch_text_embedding, device, decoder=None, num_test=None):\n",
    "    clip_model.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    total_clip_score = 0\n",
    "\n",
    "    for idx, (X, y) in enumerate(tqdm(test_dataloader)):\n",
    "        # y: label dict\n",
    "        text = y['text']\n",
    "        text = [fetch_text_embedding(x) for x in text]\n",
    "        text_embedding = torch.stack(text).to(device)\n",
    "\n",
    "        # ecgs: (gen_B, L, C)\n",
    "        X = X.to(device)\n",
    "        if decoder:\n",
    "            X = decoder(X)\n",
    "\n",
    "        signal_embedding = clip_model.encode_signal(X)\n",
    "\n",
    "        # signal features: (B, 1536)\n",
    "        signal_features = clip_model.ecg_projector(signal_embedding)\n",
    "        # text features:  (B, 1536)\n",
    "        text_features = clip_model.text_projector(text_embedding)\n",
    "\n",
    "        # normalized features\n",
    "        signal_features = signal_features / signal_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity\n",
    "        batch_clip_score = torch.trace(signal_features @ text_features.t()) \n",
    "\n",
    "        total_clip_score += batch_clip_score\n",
    "\n",
    "\n",
    "    mean_clip_score = total_clip_score / len(test_dataloader.dataset)\n",
    "\n",
    "    return mean_clip_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute CLIP Score on PTBXL (training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 341/341 [02:39<00:00,  2.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.5043, device='cuda:7')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLIP_Score(clip_model=clip_model, test_dataloader=dataloader, fetch_text_embedding=fetch_text_embedding_ptbxl, device=device, decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0058)\n"
     ]
    }
   ],
   "source": [
    "signal_features = torch.randn((256, 64))\n",
    "text_features = torch.randn((256, 64))\n",
    "# text_features = signal_features\n",
    "\n",
    "signal_features = signal_features / signal_features.norm(dim=-1, keepdim=True)\n",
    "text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# cosine similarity\n",
    "batch_clip_score = torch.trace(signal_features @ text_features.t()) \n",
    "print(batch_clip_score / 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.mimic_iv_ecg_dataset import VAE_MIMIC_IV_ECG_Dataset\n",
    "\n",
    "MIMIC_VAE_PATH = '/data/0shared/laiyongfan/data_text2ecg/mimic_vae'\n",
    "mimic_dataset = VAE_MIMIC_IV_ECG_Dataset(MIMIC_VAE_PATH)\n",
    "mimic_dataloader = DataLoader(mimic_dataset, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict_mimic = pd.read_csv('/data/0shared/chenjiabo/DiffuSETS/data/mimic_iv_text_embed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_text_embedding_mimic_report_0(text: str):\n",
    "    text = text.split('|')[0]\n",
    "    if len(text) > 0 and text[-1] != '.':\n",
    "        text += '.'\n",
    "    try:\n",
    "        text_embed = embedding_dict_mimic.loc[embedding_dict_mimic['text'] == text, 'embed'].values[0]\n",
    "        text_embed = eval(text_embed)\n",
    "    except IndexError:\n",
    "        text_embed = embedding_dict_mimic.iloc[-1]['embed']\n",
    "        text_embed = eval(text_embed)\n",
    "        # print(1, text)\n",
    "    return torch.tensor(text_embed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3104/3104 [1:06:28<00:00,  1.28s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.3333, device='cuda:7')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLIP_Score(clip_model=clip_model, test_dataloader=mimic_dataloader, fetch_text_embedding=fetch_text_embedding_mimic_report_0, device=device, decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n"
     ]
    }
   ],
   "source": [
    "for idx, (X, y) in enumerate(mimic_dataloader):\n",
    "    text = y['text']\n",
    "    text = [fetch_text_embedding_mimic_report_0(x) for x in text]\n",
    "    text_embedding = torch.stack(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "nr: 030315 }lder:  87 }r k\n"
     ]
    }
   ],
   "source": [
    "for idx, (X, y) in enumerate(dataloader):\n",
    "    text = y['text']\n",
    "    text = [fetch_text_embedding_ptbxl(x) for x in text]\n",
    "    text_embedding = torch.stack(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
