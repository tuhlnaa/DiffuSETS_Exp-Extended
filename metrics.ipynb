{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiffuSETS metric scripts\n",
    "\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiyongfan/miniconda3/envs/text2ecg/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "from diffusers import DDPMScheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from unet.conditional_unet_patient_3 import ECGconditional\n",
    "from vae.vae_model import VAE_Decoder\n",
    "from clip.clip_model import CLIP\n",
    "from dataset.ptbxl_dataset import PtbxlDataset, PtbxlDataset_VAE\n",
    "from dataset.mimic_iv_ecg_dataset import MIMIC_IV_ECG_Dataset, VAE_MIMIC_IV_ECG_Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "PTB_PATH = '/data/0shared/laiyongfan/data_text2ecg/ptb-xl/'\n",
    "PTB_VAE_PATH = '/data/0shared/laiyongfan/data_text2ecg/ptb-xl_vae'\n",
    "MIMIC_PATH = '/data1_science/1shared/physionet.org/files/mimic-iv-ecg/1.0/mimic-iv-ecg-diagnostic-electrocardiogram-matched-subset-1.0'\n",
    "MIMIC_VAE_PATH = '/data/0shared/laiyongfan/data_text2ecg/mimic_vae'\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICE'] = '7'\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP model\n",
    "# model under clip_3 folder is trained on mimic with embed dim 64\n",
    "clip_model_root = '/home/laiyongfan/text2ecg/checkpoints/clip_4/CLIP_model_ep9.pth'\n",
    "clip_model = CLIP(embed_dim=64)\n",
    "clip_model_weight = torch.load(clip_model_root, map_location=device)\n",
    "clip_model.load_state_dict(clip_model_weight)\n",
    "clip_model.eval()\n",
    "clip_model = clip_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/0shared/chenjiabo/DiffuSETS/weights/ablation_new/DDPM_1111/test/ecg2chan_input20K_1000ts_72.pth\n"
     ]
    }
   ],
   "source": [
    "# UNET\n",
    "n_channels = 4\n",
    "num_train_steps = 1000\n",
    "diffused_model = DDPMScheduler(num_train_timesteps=num_train_steps, beta_start=0.00085, beta_end=0.0120)\n",
    "unet = ECGconditional(num_train_steps, resolution=128, kernel_size=7, num_levels=5, n_channels=n_channels)\n",
    "unet_weights_dir = '/data/0shared/chenjiabo/DiffuSETS/weights/ablation_new/DDPM_1111/test/' \n",
    "files = os.listdir(unet_weights_dir)\n",
    "max_id = -1\n",
    "latest_weight_file = None\n",
    "for file in files:\n",
    "    match = re.search(r'ecg2chan_input20K_1000ts_(\\d+)\\.pth', file)\n",
    "    if match:\n",
    "        file_id = int(match.group(1))\n",
    "        if file_id > max_id:\n",
    "            max_id = file_id\n",
    "            latest_weight_file = file\n",
    "unet_weights_path = unet_weights_dir + f'ecg2chan_input20K_1000ts_{max_id}.pth'\n",
    "print(unet_weights_path)\n",
    "\n",
    "unet.load_state_dict(torch.load(unet_weights_path, map_location=device))\n",
    "unet.eval()\n",
    "unet = unet.to(device)\n",
    "diffused_model.set_timesteps(1000)\n",
    "\n",
    "decoder = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE\n",
    "decoder = VAE_Decoder()\n",
    "# VAE_path\n",
    "vae_path = '/home/laiyongfan/text2ecg/checkpoints/vae_1/VAE_model_ep9.pth'\n",
    "checkpoint = torch.load(vae_path, map_location=device)\n",
    "decoder.load_state_dict(checkpoint['decoder'])\n",
    "decoder.eval()\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read features from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_features(features_path):\n",
    "    with open(features_path + '/features.json', 'r') as file:\n",
    "        features_dict = json.load(file)\n",
    "        tensor_features = ['text_embed', 'Ori Latent', 'Gen Latent']\n",
    "        for key in features_dict:\n",
    "            if key in tensor_features:\n",
    "                features_dict[key] = eval(features_dict[key])\n",
    "    return features_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDPM inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generation_from_net(diffused_model, net, text_embedding, condition, batch_size, device, dim=128, decoder=None):\n",
    "    net.eval()\n",
    "    xi = torch.randn(batch_size, 4, dim)\n",
    "    xi = xi.to(device)\n",
    "    for _, i in enumerate(diffused_model.timesteps):\n",
    "        t = i*torch.ones(batch_size, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # change this line to fit your unet \n",
    "            if condition:\n",
    "                noise_predict = net(xi, t, text_embedding, condition)\n",
    "            if not condition:\n",
    "                noise_predict = net(xi, t, text_embedding)\n",
    "\n",
    "            xi = diffused_model.step(model_output=noise_predict, \n",
    "                                     timestep=i, \n",
    "                                     sample=xi)['prev_sample']\n",
    "\n",
    "    if decoder:\n",
    "        xi = decoder(xi)\n",
    "\n",
    "    # xi: (B, L, C) whether using VAE decoder or not\n",
    "    return xi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch text embedding\n",
    "\n",
    "Prototype:\n",
    "```python\n",
    "# Step 0: load the embedding dict\n",
    "embedding_dict = pd.read_csv(path, low_memory=False)\n",
    "\n",
    "def fetch_embedding_xxx(text: str):\n",
    "    # Step 1: transform text to key-form that can be inquired from table\n",
    "    text = get_key_text(text)\n",
    "\n",
    "    # Step 2: Inquire from table\n",
    "    try:\n",
    "        text_embed = embedding_dict.loc[embedding_dict['xxx'] == text, 'embed'].values[0]\n",
    "        text_embed = eval(text_embed)\n",
    "    except IndexError:\n",
    "        # Special case\n",
    "        text_embed = special_case()\n",
    "    \n",
    "    # Step 3: Convert to torch.Tensor\n",
    "    return torch.tensor(text_embed)\n",
    "\n",
    "# Usage:\n",
    "for idx, (X, y) in enumerate(dataloader):\n",
    "    text = y['text']\n",
    "    text = [fetch_text_embedding_mimic_report_0(x) for x in text]\n",
    "    # text_embedding: (B, 1536)\n",
    "    text_embedding = torch.stack(text).to(device)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbxl_embedding_dict = pd.read_csv('/data/0shared/chenjiabo/DiffuSETS/data/ptbxl_database_embed.csv', low_memory=False)[['ecg_id', 'text_embed']]\n",
    "ptbxl_original_sheet = pd.read_csv('/data/0shared/laiyongfan/data_text2ecg/ptb-xl/ptbxl_database.csv', low_memory=False)\n",
    "ptbxl_embedding_dict = pd.merge(ptbxl_embedding_dict, ptbxl_original_sheet)\n",
    "\n",
    "# converting ptbxl text to ada embedding\n",
    "def fetch_text_embedding_ptbxl(text:str):\n",
    "    text = text.split('|')[0]\n",
    "    text = text.replace('The report of the ECG is that ', '')\n",
    "    try:\n",
    "        text_embed = ptbxl_embedding_dict.loc[ptbxl_embedding_dict['report'] == text, 'text_embed'].values[0]\n",
    "        text_embed = eval(text_embed)\n",
    "    except IndexError:\n",
    "        text_embed = [0] * 1536\n",
    "        print(text)\n",
    "    return torch.tensor(text_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_embedding_dict = pd.read_csv('/data/0shared/chenjiabo/DiffuSETS/data/mimic_iv_text_embed.csv')\n",
    "\n",
    "# converting mimic label text into embedding\n",
    "# NOTE: ONLY use report 0\n",
    "def fetch_text_embedding_mimic_report_0(text: str):\n",
    "    text = text.split('|')[0]\n",
    "    if len(text) > 0 and text[-1] != '.':\n",
    "        text += '.'\n",
    "    try:\n",
    "        text_embed = mimic_embedding_dict.loc[mimic_embedding_dict['text'] == text, 'embed'].values[0]\n",
    "        text_embed = eval(text_embed)\n",
    "    except IndexError:\n",
    "        text_embed = mimic_embedding_dict.iloc[-1]['embed']\n",
    "        text_embed = eval(text_embed)\n",
    "        print(1, text)\n",
    "    return torch.tensor(text_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Score\n",
    "\n",
    "Compare the cosine similarity between generated ecg embedding and label text embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbxl_vae_dataset = PtbxlDataset_VAE(PTB_VAE_PATH)\n",
    "ptbxl_vae_dataloader = DataLoader(ptbxl_vae_dataset, batch_size=1, shuffle=True)\n",
    "mimic_vae_dataset = VAE_MIMIC_IV_ECG_Dataset(MIMIC_VAE_PATH)\n",
    "mimic_vae_dataloader = DataLoader(mimic_vae_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating ecg during test\n",
    "@torch.no_grad()\n",
    "def CLIP_Score(clip_model, diffused_model, unet, test_dataloader, fetch_func, device, gen_batch, decoder=None, num_test=None, use_condition=True):\n",
    "    total_clip_score = 0\n",
    "    if num_test is None:\n",
    "        num_test = len(test_dataloader)\n",
    "\n",
    "    with tqdm(total=num_test) as pbar:\n",
    "        for idx, (X, y) in enumerate(test_dataloader):\n",
    "            # y: label dict, one label during one generation\n",
    "            text = y['text'][0]\n",
    "\n",
    "            # text to embedding transform\n",
    "            # unet text embedding: (gen_B, 1, 1536)\n",
    "            text_embedding = fetch_func(text).to(device)\n",
    "            unet_text_embedding = text_embedding.repeat((gen_batch, 1, 1))\n",
    "\n",
    "            # condition part, value: (gen_B, 1, 1)\n",
    "            gender = 1 if y['gender'] == 'M' else 0\n",
    "            gender = torch.tensor([gender])\n",
    "            condition = {'gender': gender, \n",
    "                         'age': y['age'], \n",
    "                         'heart rate': y['hr']}\n",
    "\n",
    "            for key in condition:\n",
    "                condition[key] = np.array([condition[key]])\n",
    "                condition[key] = np.repeat(condition[key][np.newaxis, :], gen_batch, axis=0)\n",
    "                condition[key] = torch.Tensor(condition[key])\n",
    "                condition[key] = condition[key].to(device)\n",
    "            \n",
    "            if not use_condition:\n",
    "                condition = False\n",
    "            \n",
    "            # Generating a bunch of ECGs according to the same text\n",
    "            # ecgs: (gen_B, L, C)\n",
    "            ecgs = generation_from_net(diffused_model=diffused_model, \n",
    "                                       net=unet, \n",
    "                                       text_embedding=unet_text_embedding, \n",
    "                                       condition=condition, \n",
    "                                       batch_size=gen_batch, \n",
    "                                       device=device, \n",
    "                                       decoder=decoder)\n",
    "\n",
    "            signal_embedding = clip_model.encode_signal(ecgs)\n",
    "\n",
    "            # signal features: (gen_B, embed_dim)\n",
    "            signal_features = clip_model.ecg_projector(signal_embedding)\n",
    "            # text features: (1, embed_dim) -> (gen_B, embed_dim)\n",
    "            text_features = clip_model.text_projector(text_embedding)\n",
    "            text_features = text_features.repeat((gen_batch, 1))\n",
    "\n",
    "            # normalized features\n",
    "            signal_features = signal_features / signal_features.norm(dim=-1, keepdim=True)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # cosine similarity\n",
    "            sample_clip_score = torch.trace(signal_features @ text_features.t()) / gen_batch\n",
    "\n",
    "            total_clip_score += sample_clip_score\n",
    "\n",
    "            pbar.update(1)\n",
    "            if idx == num_test - 1:\n",
    "                break\n",
    "\n",
    "    mean_clip_score = total_clip_score / num_test\n",
    "\n",
    "    return mean_clip_score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:20<00:00, 14.08s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.1472, device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CLIP Score on MIMIC VAE\n",
    "CLIP_Score(clip_model=clip_model, \n",
    "           diffused_model=diffused_model, \n",
    "           unet=unet, \n",
    "           test_dataloader=mimic_vae_dataloader, \n",
    "           fetch_func=fetch_text_embedding_mimic_report_0, \n",
    "           device=device, \n",
    "           gen_batch=64, \n",
    "           decoder=decoder, \n",
    "           num_test=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:16<00:00, 13.68s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.1065, device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CLIP Score on Ptbxl VAE\n",
    "CLIP_Score(clip_model=clip_model, \n",
    "           diffused_model=diffused_model, \n",
    "           unet=unet, \n",
    "           test_dataloader=ptbxl_vae_dataloader, \n",
    "           fetch_func=fetch_text_embedding_ptbxl, \n",
    "           device=device, \n",
    "           gen_batch=64, \n",
    "           decoder=decoder, \n",
    "           num_test=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def CLIP_Score_saved_samples(sample_dir:str, clip_model, decoder, device):\n",
    "    \"\"\" \n",
    "    CLIP Score on saved samples\n",
    "\n",
    "    sample_dir: path to the sample directory\\n\n",
    "    /path/to/sample_dir\n",
    "       |-001\\n\n",
    "       |-002\\n\n",
    "       ...\\n \n",
    "    \"\"\"\n",
    "    total_clip_score = 0\n",
    "    for idx, root in enumerate(tqdm(os.listdir(sample_dir))):\n",
    "        feature_dict = read_features(os.path.join(sample_dir, root))\n",
    "        gen_batch = feature_dict['batch']\n",
    "\n",
    "        # text_embedding: (gen_B, 1536)\n",
    "        text_embedding = feature_dict['text_embed']\n",
    "        text_embedding = torch.tensor(text_embedding).repeat((gen_batch, 1)).to(device)\n",
    "\n",
    "        # ecg_latent: (gen_B, 4, 128)\n",
    "        ecg_latent = feature_dict['Gen Latent']\n",
    "        ecg_latent = torch.tensor(ecg_latent).to(device)\n",
    "\n",
    "        # generated ECGs: (gen_B, L, C)\n",
    "        ecgs = decoder(ecg_latent)\n",
    "\n",
    "        signal_embedding = clip_model.encode_signal(ecgs)\n",
    "\n",
    "        # signal features: (gen_B, embed_dim)\n",
    "        signal_features = clip_model.ecg_projector(signal_embedding)\n",
    "        # text features: (1, embed_dim) -> (gen_B, embed_dim)\n",
    "        text_features = clip_model.text_projector(text_embedding)\n",
    "        text_features = text_features.repeat((gen_batch, 1))\n",
    "\n",
    "        # normalized features\n",
    "        signal_features = signal_features / signal_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity\n",
    "        sample_clip_score = torch.trace(signal_features @ text_features.t())\n",
    "\n",
    "        total_clip_score += sample_clip_score\n",
    "\n",
    "    total_num = gen_batch * (idx + 1)\n",
    "\n",
    "    return {'CLIP': total_clip_score / total_num, 'num_samples': total_num}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:16<00:00,  5.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CLIP': tensor(0.0989, device='cuda:0'), 'num_samples': 6400}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLIP_Score_saved_samples(sample_dir='./sample/', clip_model=clip_model, decoder=decoder, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Classifier (Zero shot)\n",
    "\n",
    "Compare the cosine similarity between generated ecg embedding and given text label embeddings\n",
    "\n",
    "Distribute ecg to the class whose embeddding is most similar \n",
    "\n",
    "Directly read generated ecg and the relating label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_dataset = \n",
    "classifier_dataloader = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Assign classes to ecg according to the text\n",
    "def text_to_class(text:str):\n",
    "    pass\n",
    "\n",
    "# Get all the super-class features\n",
    "# 1. get the ada embedding of classes\n",
    "# 2. use clip model to get the feature\n",
    "# A tensor with shape (num_class, 1536)\n",
    "def fetch_class_features(clip_model):\n",
    "    pass\n",
    "\n",
    "class_features = fetch_class_features(clip_model=clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def CLIP_Classifier(clip_model, class_features, test_dataloader):\n",
    "    correct = 0 \n",
    "    label_all = []\n",
    "    logits_all = []\n",
    "    for idx, (X, y) in enumerate(test_dataloader):\n",
    "        label = [text_to_class(x) for x in y['text']]\n",
    "        \n",
    "        signal_embedding = clip_model.encode_signal(X)\n",
    "        signal_features = clip_model.ecg_projector(signal_embedding)\n",
    "\n",
    "        # X: (B, 1536) y: (num_class, 1536) X @ y.t(): (B, num_class)\n",
    "        logits = signal_features @ class_features.t()\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "\n",
    "        correct += torch.sum((label == pred))\n",
    "        logits_all.append(logits.reshape(-1, ))\n",
    "        label_all.append(label)\n",
    "    \n",
    "    acc = correct / len(test_dataloader.dataset)\n",
    "    logits_all = torch.concat(logits_all)\n",
    "    label_all = torch.concat(label_all)\n",
    "\n",
    "    return acc, label_all, logits_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FID Score\n",
    "\n",
    "Compute the distance between the generated ecg embedding and real ecg embedding\n",
    "\n",
    "Have to compute between two datasets\n",
    "\n",
    "**INPUT**: Stacked embeddings generated from CLIP model\n",
    "\n",
    "**INPUT SHAPE**: (num_ecgs, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_feature_matrix(sample_dir:str, clip_model, device, decoder, use_all_batch=True):\n",
    "    \"\"\" \n",
    "    Generating feature matrix from experiment folder\n",
    "    sample_dir: path to the sample directory\\n\n",
    "    /path/to/sample_dir\n",
    "       |-001\\n\n",
    "       |-002\\n\n",
    "       ...\\n \n",
    "    use_all_batch: whether to use whole batch, \n",
    "    if not, only sample one piece of ecg from each generation folder.\\n\n",
    "    return: dict of `gen` and `real`, which contains feature matrix of shape (num_samples, feature_dim)\n",
    "    \"\"\"\n",
    "    M_gen = []\n",
    "    M_real = []\n",
    "    for idx, root in enumerate(tqdm(os.listdir(sample_dir))):\n",
    "        feature_dict = read_features(os.path.join(sample_dir, root))\n",
    "\n",
    "        # ecg_latent: (gen_B, 4, 128)\n",
    "        gen_latent = feature_dict['Gen Latent']\n",
    "        gen_latent = torch.tensor(gen_latent).to(device)\n",
    "\n",
    "        # generated ECGs: (gen_B, L, C)\n",
    "        gen_ecgs = decoder(gen_latent)\n",
    "        \n",
    "        # gen_ecg_features: (gen_B, feature_dim) or (1, feature_dim)\n",
    "        gen_ecg_embedding = clip_model.encode_signal(gen_ecgs)\n",
    "        gen_ecg_features = clip_model.ecg_projector(gen_ecg_embedding)\n",
    "\n",
    "        if not use_all_batch:\n",
    "            gen_ecg_features = gen_ecg_features[0].unsqueeze(0)\n",
    "        gen_ecg_features = gen_ecg_features.cpu()\n",
    "\n",
    "        M_gen.append(gen_ecg_features)\n",
    "\n",
    "        # ori_latent: (1, 4, 128)\n",
    "        ori_latent = feature_dict['Ori Latent']\n",
    "        ori_latent = torch.tensor(ori_latent).to(device)\n",
    "        ori_ecgs = decoder(ori_latent)\n",
    "        ori_ecg_embedding = clip_model.encode_signal(ori_ecgs)\n",
    "        # ori_ecg_feature: (1, feature_dim)\n",
    "        ori_ecg_features = clip_model.ecg_projector(ori_ecg_embedding)\n",
    "        ori_ecg_features = ori_ecg_features.cpu()\n",
    "        M_real.append(ori_ecg_features)\n",
    "\n",
    "    M_gen = torch.concat(M_gen)\n",
    "    M_real = torch.concat(M_real)\n",
    "\n",
    "    # M_gen: (num_samples, num_features)\n",
    "    return {'gen': M_gen, 'real': M_real}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:18<00:00,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6400, 64]) torch.Size([100, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "state = generate_feature_matrix(sample_dir='./sample/', clip_model=clip_model, device=device, decoder=decoder, use_all_batch=True)\n",
    "M_gen, M_real = state['gen'], state['real']\n",
    "print(M_gen.shape, M_real.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FID_score(M1: torch.Tensor, M2: torch.Tensor):\n",
    "    M1, M2 = M1.numpy(), M2.numpy()\n",
    "    mu1, sigma1 = M1.mean(axis=0), np.cov(M1, rowvar=False)\n",
    "    mu2, sigma2 = M2.mean(axis=0), np.cov(M2, rowvar=False)\n",
    "\n",
    "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "    # calculate sqrt of product between cov\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    # check and correct imaginary numbers from sqrt\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    # calculate score\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339490635.045777 6.848712837192828\n"
     ]
    }
   ],
   "source": [
    "fid_score = FID_score(M_real, M_gen) \n",
    "num_samples = M_real.shape[0]\n",
    "scaler = FID_score(M_real[:num_samples // 2], M_real[num_samples // 2:])\n",
    "\n",
    "# From ME-GAN, can produce result more stable to num_samples (and smaller result)\n",
    "r_FID = fid_score / scaler\n",
    "\n",
    "print(fid_score, r_FID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "\n",
    "More samples, more close to real performance  \n",
    "\n",
    "**If M_real and M_gen are both sampled from N(0, 1)** \n",
    "\n",
    "when num_samples is 2048: FID = 257, r_FID = 0.50; \n",
    "\n",
    "when num_samples is 20480: FID = 25, r_FID = 0.50;\n",
    "\n",
    "**If M_real ~ N(0, 1) and M_gen ~ N(1, 4)** \n",
    "\n",
    "when num_samples is 2048: FID = 2567, r_FID = 5; √\n",
    "\n",
    "when num_samples is 20480: FID = 2099, r_FID = 40;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall\n",
    "\n",
    "Use k-NN to construct manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManifoldDetector():\n",
    "    def __init__(self, data: torch.Tensor, k=3):\n",
    "        self.k = k\n",
    "        self.data = data\n",
    "\n",
    "        # Compute pairwise distances\n",
    "        distances = torch.sqrt(torch.sum((self.data.unsqueeze(1) - self.data.unsqueeze(0))**2, dim=2))\n",
    "\n",
    "        # Get indices of k-nearest neighbors\n",
    "        _, indices = torch.topk(distances, k=self.k + 1, dim=1, largest=False)\n",
    "        indices = indices[:, 1:]  # Exclude the point itself\n",
    "\n",
    "        # Compute radius as the distance to the k-th nearest neighbor\n",
    "        self.radii = torch.gather(distances, 1, indices[:, -1].view(-1, 1))\n",
    "\n",
    "def is_in_manifold(test_point: torch.Tensor, manifold_detector: ManifoldDetector):\n",
    "    distances = torch.sqrt(torch.sum((manifold_detector.data - test_point)**2, dim=1))\n",
    "    is_inside = distances <= manifold_detector.radii.squeeze()\n",
    "    return is_inside.any()\n",
    "\n",
    "def points_in_manifold(test_points: torch.Tensor, manifold_detector: ManifoldDetector):\n",
    "    count = 0\n",
    "    for point in test_points:\n",
    "       count += is_in_manifold(point, manifold_detector) \n",
    "\n",
    "    return count\n",
    "\n",
    "def precision_recall(M_g, M_r, k=3):\n",
    "    \"\"\" \n",
    "    Compute the precision and recall value for generation result\\n\n",
    "    M_g: feature matrix of generated ECG\\n\n",
    "    M_r: feature matrix of real ECG\\n\n",
    "    k: using distance from k nearest neighborhood to constuct manifold \n",
    "    \"\"\"\n",
    "    print(\"Initializing...\")\n",
    "    manifold_detector_g = ManifoldDetector(M_g, k)\n",
    "    manifold_detector_r = ManifoldDetector(M_r, k)\n",
    "\n",
    "    state = {}\n",
    "    print(\"computing precision...\")\n",
    "    num_precision = points_in_manifold(M_g, manifold_detector_r)\n",
    "    state['precision'] = num_precision / M_g.shape[0]\n",
    "\n",
    "    print(\"computing recall...\")\n",
    "    num_recall = points_in_manifold(M_r, manifold_detector_g)\n",
    "    state['recall'] = num_recall / M_r.shape[0] \n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n",
      "computing precision...\n",
      "computing recall...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': tensor(0.9906), 'recall': tensor(0.3000)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall(M_g=M_gen, M_r=M_real)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
