{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from classification.classifier import ResNetECG \n",
    "\n",
    "ecg_length = 1024 \n",
    "ecg_channel = 12\n",
    "\n",
    "x = torch.rand((4, ecg_length, ecg_channel)) \n",
    "model_res = ResNetECG(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_res(x).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: resnet.first_conv.conv.weight | Matrix size: torch.Size([64, 12, 16])\n",
      "Layer: resnet.first_bn.weight | Matrix size: torch.Size([64])\n",
      "Layer: resnet.stage_list.0.block_list.0.bn1.weight | Matrix size: torch.Size([64])\n",
      "Layer: resnet.stage_list.0.block_list.0.conv1.conv.weight | Matrix size: torch.Size([64, 64, 1])\n",
      "Layer: resnet.stage_list.0.block_list.0.bn2.weight | Matrix size: torch.Size([64])\n",
      "Layer: resnet.stage_list.0.block_list.0.conv2.conv.weight | Matrix size: torch.Size([64, 16, 16])\n",
      "Layer: resnet.stage_list.0.block_list.0.bn3.weight | Matrix size: torch.Size([64])\n",
      "Layer: resnet.stage_list.0.block_list.0.conv3.conv.weight | Matrix size: torch.Size([64, 64, 1])\n",
      "Layer: resnet.stage_list.0.block_list.0.se_fc1.weight | Matrix size: torch.Size([32, 64])\n",
      "Layer: resnet.stage_list.0.block_list.0.se_fc2.weight | Matrix size: torch.Size([64, 32])\n",
      "Layer: resnet.stage_list.0.block_list.1.bn1.weight | Matrix size: torch.Size([64])\n",
      "Layer: resnet.stage_list.0.block_list.1.conv1.conv.weight | Matrix size: torch.Size([64, 64, 1])\n",
      "Layer: resnet.stage_list.0.block_list.1.bn2.weight | Matrix size: torch.Size([64])\n",
      "Layer: resnet.stage_list.0.block_list.1.conv2.conv.weight | Matrix size: torch.Size([64, 16, 16])\n",
      "Layer: resnet.stage_list.0.block_list.1.bn3.weight | Matrix size: torch.Size([64])\n",
      "Layer: resnet.stage_list.0.block_list.1.conv3.conv.weight | Matrix size: torch.Size([64, 64, 1])\n",
      "Layer: resnet.stage_list.0.block_list.1.se_fc1.weight | Matrix size: torch.Size([32, 64])\n",
      "Layer: resnet.stage_list.0.block_list.1.se_fc2.weight | Matrix size: torch.Size([64, 32])\n",
      "Layer: resnet.stage_list.1.block_list.0.bn1.weight | Matrix size: torch.Size([64])\n",
      "Layer: resnet.stage_list.1.block_list.0.conv1.conv.weight | Matrix size: torch.Size([128, 64, 1])\n",
      "Layer: resnet.stage_list.1.block_list.0.bn2.weight | Matrix size: torch.Size([128])\n",
      "Layer: resnet.stage_list.1.block_list.0.conv2.conv.weight | Matrix size: torch.Size([128, 16, 16])\n",
      "Layer: resnet.stage_list.1.block_list.0.bn3.weight | Matrix size: torch.Size([128])\n",
      "Layer: resnet.stage_list.1.block_list.0.conv3.conv.weight | Matrix size: torch.Size([128, 128, 1])\n",
      "Layer: resnet.stage_list.1.block_list.0.se_fc1.weight | Matrix size: torch.Size([64, 128])\n",
      "Layer: resnet.stage_list.1.block_list.0.se_fc2.weight | Matrix size: torch.Size([128, 64])\n",
      "Layer: resnet.stage_list.1.block_list.1.bn1.weight | Matrix size: torch.Size([128])\n",
      "Layer: resnet.stage_list.1.block_list.1.conv1.conv.weight | Matrix size: torch.Size([128, 128, 1])\n",
      "Layer: resnet.stage_list.1.block_list.1.bn2.weight | Matrix size: torch.Size([128])\n",
      "Layer: resnet.stage_list.1.block_list.1.conv2.conv.weight | Matrix size: torch.Size([128, 16, 16])\n",
      "Layer: resnet.stage_list.1.block_list.1.bn3.weight | Matrix size: torch.Size([128])\n",
      "Layer: resnet.stage_list.1.block_list.1.conv3.conv.weight | Matrix size: torch.Size([128, 128, 1])\n",
      "Layer: resnet.stage_list.1.block_list.1.se_fc1.weight | Matrix size: torch.Size([64, 128])\n",
      "Layer: resnet.stage_list.1.block_list.1.se_fc2.weight | Matrix size: torch.Size([128, 64])\n",
      "Layer: resnet.stage_list.2.block_list.0.bn1.weight | Matrix size: torch.Size([128])\n",
      "Layer: resnet.stage_list.2.block_list.0.conv1.conv.weight | Matrix size: torch.Size([256, 128, 1])\n",
      "Layer: resnet.stage_list.2.block_list.0.bn2.weight | Matrix size: torch.Size([256])\n",
      "Layer: resnet.stage_list.2.block_list.0.conv2.conv.weight | Matrix size: torch.Size([256, 16, 16])\n",
      "Layer: resnet.stage_list.2.block_list.0.bn3.weight | Matrix size: torch.Size([256])\n",
      "Layer: resnet.stage_list.2.block_list.0.conv3.conv.weight | Matrix size: torch.Size([256, 256, 1])\n",
      "Layer: resnet.stage_list.2.block_list.0.se_fc1.weight | Matrix size: torch.Size([128, 256])\n",
      "Layer: resnet.stage_list.2.block_list.0.se_fc2.weight | Matrix size: torch.Size([256, 128])\n",
      "Layer: resnet.stage_list.2.block_list.1.bn1.weight | Matrix size: torch.Size([256])\n",
      "Layer: resnet.stage_list.2.block_list.1.conv1.conv.weight | Matrix size: torch.Size([256, 256, 1])\n",
      "Layer: resnet.stage_list.2.block_list.1.bn2.weight | Matrix size: torch.Size([256])\n",
      "Layer: resnet.stage_list.2.block_list.1.conv2.conv.weight | Matrix size: torch.Size([256, 16, 16])\n",
      "Layer: resnet.stage_list.2.block_list.1.bn3.weight | Matrix size: torch.Size([256])\n",
      "Layer: resnet.stage_list.2.block_list.1.conv3.conv.weight | Matrix size: torch.Size([256, 256, 1])\n",
      "Layer: resnet.stage_list.2.block_list.1.se_fc1.weight | Matrix size: torch.Size([128, 256])\n",
      "Layer: resnet.stage_list.2.block_list.1.se_fc2.weight | Matrix size: torch.Size([256, 128])\n",
      "Layer: resnet.stage_list.3.block_list.0.bn1.weight | Matrix size: torch.Size([256])\n",
      "Layer: resnet.stage_list.3.block_list.0.conv1.conv.weight | Matrix size: torch.Size([512, 256, 1])\n",
      "Layer: resnet.stage_list.3.block_list.0.bn2.weight | Matrix size: torch.Size([512])\n",
      "Layer: resnet.stage_list.3.block_list.0.conv2.conv.weight | Matrix size: torch.Size([512, 16, 16])\n",
      "Layer: resnet.stage_list.3.block_list.0.bn3.weight | Matrix size: torch.Size([512])\n",
      "Layer: resnet.stage_list.3.block_list.0.conv3.conv.weight | Matrix size: torch.Size([512, 512, 1])\n",
      "Layer: resnet.stage_list.3.block_list.0.se_fc1.weight | Matrix size: torch.Size([256, 512])\n",
      "Layer: resnet.stage_list.3.block_list.0.se_fc2.weight | Matrix size: torch.Size([512, 256])\n",
      "Layer: resnet.stage_list.3.block_list.1.bn1.weight | Matrix size: torch.Size([512])\n",
      "Layer: resnet.stage_list.3.block_list.1.conv1.conv.weight | Matrix size: torch.Size([512, 512, 1])\n",
      "Layer: resnet.stage_list.3.block_list.1.bn2.weight | Matrix size: torch.Size([512])\n",
      "Layer: resnet.stage_list.3.block_list.1.conv2.conv.weight | Matrix size: torch.Size([512, 16, 16])\n",
      "Layer: resnet.stage_list.3.block_list.1.bn3.weight | Matrix size: torch.Size([512])\n",
      "Layer: resnet.stage_list.3.block_list.1.conv3.conv.weight | Matrix size: torch.Size([512, 512, 1])\n",
      "Layer: resnet.stage_list.3.block_list.1.se_fc1.weight | Matrix size: torch.Size([256, 512])\n",
      "Layer: resnet.stage_list.3.block_list.1.se_fc2.weight | Matrix size: torch.Size([512, 256])\n",
      "Layer: resnet.stage_list.3.block_list.2.bn1.weight | Matrix size: torch.Size([512])\n",
      "Layer: resnet.stage_list.3.block_list.2.conv1.conv.weight | Matrix size: torch.Size([512, 512, 1])\n",
      "Layer: resnet.stage_list.3.block_list.2.bn2.weight | Matrix size: torch.Size([512])\n",
      "Layer: resnet.stage_list.3.block_list.2.conv2.conv.weight | Matrix size: torch.Size([512, 16, 16])\n",
      "Layer: resnet.stage_list.3.block_list.2.bn3.weight | Matrix size: torch.Size([512])\n",
      "Layer: resnet.stage_list.3.block_list.2.conv3.conv.weight | Matrix size: torch.Size([512, 512, 1])\n",
      "Layer: resnet.stage_list.3.block_list.2.se_fc1.weight | Matrix size: torch.Size([256, 512])\n",
      "Layer: resnet.stage_list.3.block_list.2.se_fc2.weight | Matrix size: torch.Size([512, 256])\n",
      "Layer: head.weight | Matrix size: torch.Size([10, 512])\n",
      "Total number of parameters in the model: 3368202\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_res.named_parameters():\n",
    "    if \"weight\" in name:  # Filter only weight matrices\n",
    "        print(f\"Layer: {name} | Matrix size: {param.size()}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model_res.parameters())\n",
    "print(f\"Total number of parameters in the model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification.classifier import TransformerECG \n",
    "\n",
    "# Example usage\n",
    "embed_size = 256\n",
    "num_heads = 4\n",
    "ff_hidden_size = 512\n",
    "num_layers = 3\n",
    "dropout = 0.1\n",
    "\n",
    "# Create transformer encoder\n",
    "encoder = TransformerECG(embed_size, ecg_length, ecg_channel, num_heads, ff_hidden_size, num_layers, dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: embedding.weight | Matrix size: torch.Size([256, 12])\n",
      "Layer: layers.0.self_attn.in_proj_weight | Matrix size: torch.Size([768, 256])\n",
      "Layer: layers.0.self_attn.out_proj.weight | Matrix size: torch.Size([256, 256])\n",
      "Layer: layers.0.feed_forward.fc1.weight | Matrix size: torch.Size([512, 256])\n",
      "Layer: layers.0.feed_forward.fc2.weight | Matrix size: torch.Size([256, 512])\n",
      "Layer: layers.0.layer_norm1.weight | Matrix size: torch.Size([256])\n",
      "Layer: layers.0.layer_norm2.weight | Matrix size: torch.Size([256])\n",
      "Layer: layers.1.self_attn.in_proj_weight | Matrix size: torch.Size([768, 256])\n",
      "Layer: layers.1.self_attn.out_proj.weight | Matrix size: torch.Size([256, 256])\n",
      "Layer: layers.1.feed_forward.fc1.weight | Matrix size: torch.Size([512, 256])\n",
      "Layer: layers.1.feed_forward.fc2.weight | Matrix size: torch.Size([256, 512])\n",
      "Layer: layers.1.layer_norm1.weight | Matrix size: torch.Size([256])\n",
      "Layer: layers.1.layer_norm2.weight | Matrix size: torch.Size([256])\n",
      "Layer: layers.2.self_attn.in_proj_weight | Matrix size: torch.Size([768, 256])\n",
      "Layer: layers.2.self_attn.out_proj.weight | Matrix size: torch.Size([256, 256])\n",
      "Layer: layers.2.feed_forward.fc1.weight | Matrix size: torch.Size([512, 256])\n",
      "Layer: layers.2.feed_forward.fc2.weight | Matrix size: torch.Size([256, 512])\n",
      "Layer: layers.2.layer_norm1.weight | Matrix size: torch.Size([256])\n",
      "Layer: layers.2.layer_norm2.weight | Matrix size: torch.Size([256])\n",
      "Layer: head.weight | Matrix size: torch.Size([2, 262144])\n",
      "Total number of parameters in the model: 2108930\n"
     ]
    }
   ],
   "source": [
    "for name, param in encoder.named_parameters():\n",
    "    if \"weight\" in name:  # Filter only weight matrices\n",
    "        print(f\"Layer: {name} | Matrix size: {param.size()}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in encoder.parameters())\n",
    "print(f\"Total number of parameters in the model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_res(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25816 24424 1.0569931215198165\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dataset.mimic_iv_ecg_dataset import DictDataset \n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "normal_dataset = DictDataset(path='./prerequisites/clf_data/mimic_vae_clf_{af}_train_cmp.pt') \n",
    "dataloader = DataLoader(normal_dataset, 16)\n",
    "\n",
    "normal_cnt = 0\n",
    "abnormal_cnt = 0 \n",
    "for _, y in normal_dataset: \n",
    "    if y['label'] == 1:\n",
    "        normal_cnt += 1\n",
    "    else:\n",
    "        abnormal_cnt += 1\n",
    "\n",
    "\n",
    "print(normal_cnt, abnormal_cnt, normal_cnt / abnormal_cnt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Normal   Abnormal   Ratio       cmp    Ratio\n",
    "train   25816       14184   1.82      22376     1.15\n",
    "valid   3207        1793    1.78\n",
    "test    3255        1745    1.86\n",
    "\n",
    "        Non-AF        AF    Ratio\n",
    "train   35791       4209    8.50      34929     1.02\n",
    "valid   4380        620     7.06\n",
    "test    4501        499     9.02\n",
    "\n",
    "\n",
    "        Non-AF       PVC    Ratio\n",
    "train   37428       2572   14.55      38412     0.97 \n",
    "valid   4707        293    16.06\n",
    "test    4716        284    16.60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for (X, y) in dataloader: \n",
    "    print(y['label']) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5333333333333333"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.metrics import f1_score \n",
    "\n",
    "a = np.array([1, 0, 1])\n",
    "b = np.array([1, 1, 1]) \n",
    "\n",
    "f1_score(a, b, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "(10240, 4, 128)\n",
      "50240\n"
     ]
    }
   ],
   "source": [
    "# Combine Dataset \n",
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "exp_type = 'normal'\n",
    "ori_dict_path = f'./prerequisites/clf_data/mimic_vae_clf_{exp_type}_train.pt'\n",
    "ori_dict = torch.load(ori_dict_path) \n",
    "print(len(ori_dict)) \n",
    "comp_key_offset = max(ori_dict.keys()) + 1\n",
    "\n",
    "comp_path = f'./prerequisites/clf_data/completion_{exp_type}.npy'\n",
    "comp_matrix = np.load(comp_path) \n",
    "print(comp_matrix.shape) \n",
    "\n",
    "for idx in range(comp_matrix.shape[0]): \n",
    "    new_key = comp_key_offset + idx \n",
    "    ecg = comp_matrix[idx] \n",
    "    ecg = torch.from_numpy(ecg)\n",
    "\n",
    "    value = { 'label': {'label': 0, 'text': 'cmp'}, \n",
    "                 'data': ecg}\n",
    "    ori_dict[new_key] = value \n",
    "\n",
    "print(len(ori_dict)) \n",
    "torch.save(ori_dict, f'./prerequisites/clf_data/mimic_vae_clf_{exp_type}_train_cmp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6718036529680367, 0.8229679802955664, 0.7702116300582584)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def weighted_f1_score_from_confusion_matrix(cm):\n",
    "    # Calculate precision and recall\n",
    "    # cm: pred: 0   1\n",
    "    # true  0   tn  fp\n",
    "    #       1   fn  tp\n",
    "    tp = cm[1][1]; fp = cm[0][1]; fn = cm[1][0]\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1_score_1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    tp = cm[0][0]; fp = cm[1][0]; fn = cm[0][1]\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "    f1_score_0 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    num_0 = sum(cm[0]); num_1 = sum(cm[1]) \n",
    "\n",
    "    weighted_f1 = (num_0 * f1_score_0 + num_1 * f1_score_1) / (num_0 + num_1) \n",
    "\n",
    "    return f1_score_0, f1_score_1, weighted_f1 \n",
    "\n",
    "exp_type = 'normal'\n",
    "idx = 3\n",
    "model_type = 'ResNet'\n",
    "cm_1 = np.load(f'./checkpoints/clf_{exp_type}_{model_type}_{idx}/cm.npy')\n",
    "weighted_f1_score_from_confusion_matrix(cm_1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
